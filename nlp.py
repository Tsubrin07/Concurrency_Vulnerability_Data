import spacy
import random
from spacy.util import minibatch, compounding
import chardet

# Load the pre-trained English language model
nlp = spacy.load("en_core_web_sm")


# Convert the CoNLL file to spaCy's training format
TRAIN_DATA = []
with open("C:/Users/DELL/Downloads/Thesis wife/labeledData.conll", "rb") as f:
    encoding = chardet.detect(f.read())['encoding']
with open("C:/Users/DELL/Downloads/Thesis wife/labeledData.conll", "r", encoding=encoding) as f:
    lines = f.readlines()
    words = []
    entities = []
    for line in lines:
        if line.startswith("-DOCSTART-") or line == "\n":
            if words:
                doc = nlp(" ".join(words))
                entities = [(start, end, label) for start, end, label in entities]
                doc.ents = [Span(doc, start, end, label) for start, end, label in entities]
                TRAIN_DATA.append((doc, {"entities": doc.ents}))
                words = []
                entities = []
        else:
            line = line.strip()
            if len(line.split("\t")) >= 2:
                word, label = line.split("\t")
                if label != "O":
                    if label not in ["Version", "Impact", "Reason", "Attacker", "Vector", "Result"]:
                        label = "Result"  # Replace unknown label with "Result"
                    start = len(" ".join(words)) + 1  # Add 1 for the space
                    end = start + len(word)
                    entities.append((start, end, label))
                words.append(word)

# Define the labels used in the data
LABELS = ["Version", "Impact", "Reason", "Attacker", "Vector", "Result"]

# Add the labels to the NER component
ner = nlp.get_pipe("ner")
for label in LABELS:
    ner.add_label(label)

# Train the NER model on the converted data
n_iter = 10
for i in range(n_iter):
    random.shuffle(TRAIN_DATA)
    losses = {}
    batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))
    for batch in batches:
        docs, annotations = zip(*batch)
        nlp.update(
            docs,
            annotations,
            drop=0.5,
            losses=losses,
        )
    print("Iteration {} Loss: {}".format(i, losses))

# Test the model on a new example
text = "IBM MQ Appliance 9.2 CD and 9.2 LTS is affected by a denial of service attack"
doc = nlp(text)
entities = [(ent.text, ent.label_) for ent in doc.ents if ent.label_ in LABELS]
print(entities)
print([(ent.text, ent.label_) for ent in doc.ents])
print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ in LABELS])

